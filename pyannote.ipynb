{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyaclement/audio-class/blob/main/pyannote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fs2d8otYnp7"
      },
      "source": [
        "[`pyannote.audio`](https://github.com/pyannote/pyannote-audio) is an open-source toolkit written in Python for **speaker diarization**.\n",
        "\n",
        "Based on [`PyTorch`](https://pytorch.org) machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines.\n",
        "\n",
        "`pyannote.audio` also comes with pretrained [models](https://huggingface.co/models?other=pyannote-audio-model) and [pipelines](https://huggingface.co/models?other=pyannote-audio-pipeline) covering a wide range of domains for voice activity detection, speaker segmentation, overlapped speech detection, speaker embedding reaching state-of-the-art performance for most of them.\n",
        "\n",
        "**This notebook will teach you how to apply those pretrained pipelines on your own data.**\n",
        "\n",
        "Make sure you run it using a GPU (or it might otherwise be slow...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tckHJKZnYnp7"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai082p4HYnp7"
      },
      "outputs": [],
      "source": [
        "!pip install -qq pyannote.audio==3.1.1\n",
        "!pip install -qq ipython==7.34.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FQXT5FYnp-"
      },
      "source": [
        "This nice visualization is brought to you by [`pyannote.core`](http://pyannote.github.io/pyannote-core/) and basically indicates when each speaker speaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDhZ3bXEYnp-"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Audio\n",
        "from IPython.display import Audio as IPythonAudio\n",
        "waveform, sr = Audio(mono=\"downmix\").crop(DEMO_FILE, EXCERPT)\n",
        "IPythonAudio(waveform.flatten(), rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkzox7QIYnp_"
      },
      "source": [
        "# Processing your own audio file (optional)\n",
        "\n",
        "In case you just want to go ahead with the demo file, skip this section entirely.\n",
        "\n",
        "In case you want to try processing your own audio file, proceed with running this section. It will offer you to upload an audio file (preferably a `wav` file but all formats supported by [`SoundFile`](https://pysoundfile.readthedocs.io/en/latest/) should work just fine)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmFmLzFYnp_"
      },
      "source": [
        "## Upload audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC05jFO_Ynp_"
      },
      "outputs": [],
      "source": [
        "from pyannote.core import Segment, notebook # Import 'notebook' from pyannote.core\n",
        "import google.colab\n",
        "\n",
        "own_file, _ = google.colab.files.upload().popitem()\n",
        "OWN_FILE = {'audio': own_file}\n",
        "notebook.reset() # Now 'notebook' is defined and can be used\n",
        "\n",
        "# load audio waveform and play it\n",
        "waveform, sample_rate = Audio(mono=\"downmix\")(OWN_FILE)\n",
        "IPythonAudio(data=waveform.squeeze(), rate=sample_rate, autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MclWK2GYnp_"
      },
      "source": [
        "# Speaker diarization with `pyannote.pipeline`\n",
        "\n",
        "We are about to run a full speaker diarization pipeline, that includes speaker segmentation, speaker embedding, and a final clustering step. **Brace yourself!**\n",
        "\n",
        "To load the speaker diarization pipeline,\n",
        "\n",
        "* accept the user conditions on [hf.co/pyannote/speaker-diarization-3.1](https://hf.co/pyannote/speaker-diarization-3.1)\n",
        "* accept the user conditions on [hf.co/pyannote/segmentation-3.0](https://hf.co/pyannote/segmentation-3.0)\n",
        "* login using `notebook_login` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5u7VMb-YnqB"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUq1UvoJYnqB"
      },
      "outputs": [],
      "source": [
        "# load pretrained pipeline\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "# Specify the number of expected speakers\n",
        "num_speakers = 2  # Replace with your desired number\n",
        "\n",
        "# Ensure that use_auth_token is correctly passed to from_pretrained\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=\"ADD-OWN-TOKEN\"\n",
        ")\n",
        "\n",
        "# Add a check to see if pipeline is loaded correctly.\n",
        "if pipeline is None:\n",
        "    raise ValueError(\"Pipeline failed to load. Check your internet connection and authentication token.\")\n",
        "\n",
        "# send pipeline to GPU (when available)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "# run the pipeline (with progress bar and num_speakers)\n",
        "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
        "with ProgressHook() as hook:\n",
        "    diarization = pipeline(DEMO_FILE, hook=hook, num_speakers=num_speakers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkicJRq-YnqB"
      },
      "source": [
        "That's it? Yes, that's it :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPosdyGrYnqB"
      },
      "outputs": [],
      "source": [
        "diarization"
      ]
    },
    {
      "source": [
        "\n",
        "# Convert diarization to a tab-delimited string\n",
        "def diarization_to_tsv(diarization, file_name='diarization_results.txt'):\n",
        "  \"\"\"Converts a pyannote.core.Annotation object to a tab-delimited string.\n",
        "\n",
        "  Args:\n",
        "    diarization: A pyannote.core.Annotation object containing the diarization results.\n",
        "    file_name: The name of the output file. Defaults to 'diarization_results.txt'.\n",
        "\n",
        "  Returns:\n",
        "    None. Saves the tab-delimited string to a file.\n",
        "  \"\"\"\n",
        "\n",
        "  # The with open statement is now inside the function,\n",
        "  # using the file_name parameter.\n",
        "  with open(file_name, 'w') as f:\n",
        "    for segment, track, label in diarization.itertracks(yield_label=True):\n",
        "      start_time = round(segment.start)  # Round start time to nearest whole number\n",
        "      end_time = round(segment.end)  # Round end time to nearest whole number\n",
        "      speaker_label = label\n",
        "      f.write(f\"{start_time}\\t{end_time}\\t{speaker_label}\\n\") # Removed '.3f' for whole numbers\n",
        "\n",
        "# ... (After running the diarization pipeline) ...\n",
        "\n",
        "# Save the diarization results to a tab-delimited file\n",
        "diarization_to_tsv(diarization) # Call the function with the diarization object\n",
        "\n",
        "# Download the file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('diarization_results.txt')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dU6KijMwT6sv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}